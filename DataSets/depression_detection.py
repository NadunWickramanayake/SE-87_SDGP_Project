# -*- coding: utf-8 -*-
"""Depression_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n3I_EXIfHw5jU7cZSG8xwXX6IWv40Ey6

**Import necessary libraries**
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib
import pickle

"""**Download NLTK stopwords**"""

# Download NLTK stopwords
nltk.download('stopwords')

"""**Load the dataset**"""

# Load the dataset
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/MindGuard_SDGP/Tweets.csv')

"""**Remove unnecessary columns**"""

# Remove unnecessary columns
df.drop(['textID'], axis=1, inplace=True)

"""**Clean the text data**"""

# Clean the text data
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))
    return text

import nltk
nltk.download('stopwords')

import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

df['text'] = df['text'].apply(lambda x: clean_text(x))
df['selected_text'] = df['selected_text'].apply(lambda x: clean_text(x))

"""**Map the sentiment to binary values**"""

# Map the sentiment to binary values
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

from sklearn.feature_extraction.text import TfidfVectorizer

"""**Create feature vectors**"""

import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')

# Create feature vectors
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2))
features = tfidf.fit_transform(df['text']).toarray()

from sklearn.model_selection import train_test_split

"""**Split the dataset into training and testing sets**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features, df['sentiment'], test_size=0.2, random_state=42)

"""**Train a logistic regression model**"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train, y_train)

"""**Predict the sentiment of test data**"""

# Predict the sentiment of test data
y_pred = lr.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)

"""**Calculate the accuracy**"""

print("Accuracy:", accuracy)

"""**Save the trained model**"""

# Save the trained model
joblib.dump(lr, 'sentiment_analysis_model.pkl')

"""**Print the results**"""

import pickle

# specify the file path where the model is saved
filename = 'sentiment_analysis_model.pkl'

# initialize the loaded_model variable
loaded_model = None

# load the saved model
try:
    loaded_model = pickle.load(open(filename, 'rb'))
    print("Model loaded successfully!")
except FileNotFoundError:
    print("Model not found at the specified file path.")

# print the loaded model
if loaded_model is not None and loaded_model.any():
    print(loaded_model)