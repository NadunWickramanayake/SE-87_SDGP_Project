# -*- coding: utf-8 -*-
"""Depression_Detection_New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/103kbaJm1abFlPZ1bRQvLrzObY1129g6h

**Import necessary libraries**
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib
import pickle

"""**Download NLTK stopwords**"""

# Download NLTK stopwords
nltk.download('stopwords')

"""**Load the dataset**"""

!pip install -U --no-cache-dir gdown --pre

!gdown 12Gnu6jDvK6tLkDqKGOOMzBvTKwrwDyy5

# Load the dataset
import pandas as pd
df = pd.read_csv('/content/Tweets.csv')

"""**Remove unnecessary columns**"""

# Remove unnecessary columns
df.drop(['textID'], axis=1, inplace=True)

"""**Clean the text data**"""

# Clean the text data
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))
    return text

import re
from nltk.corpus import stopwords

df['text'] = df['text'].apply(lambda x: clean_text(x))
df['selected_text'] = df['selected_text'].apply(lambda x: clean_text(x))

"""**Map the sentiment to binary values**"""

# Map the sentiment to binary values
df['sentiment'] = df['sentiment'].map({'positive': 2, 
                                       'negative': 0, 'neutral': 1})

"""**Create feature vectors**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, 
                            preprocessor=None,
                            use_idf=True, norm='l2', smooth_idf=True)
def preprocess_data(df):
    
    # Remove stop words and apply TF-IDF vectorization
    X = tfidf.fit_transform(df['text'])
    x = tfidf.transform(df['text'])
    return X

"""**Split the dataset into training and testing sets**"""

from sklearn.model_selection import train_test_split

X = preprocess_data(df) 
X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], 
                                                    test_size=0.2, 
                                                    random_state=42)

"""**Train a logistic regression model**"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

lr.fit(X_train, y_train)

"""**Predict the sentiment of test data**"""

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred = lr.predict(X_test)
acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Accuracy:", acc)
print("Confusion matrix:\n", cm)

"""**Save the trained model**"""

import pickle as pkl

with open(r"LRPWeights.pkl", "wb") as output_file:
   pkl.dump(lr, output_file)

"""**Print the results**"""

filename = '/content/LRPWeights.pkl'
loaded_model = pickle.load(open(filename, 'rb'))

def classify(prediction):
  if prediction == 0:
    return 'Negative'
  elif prediction == 1:
    return 'Neutral'
  else:
    return 'Positive'

import pickle
import numpy as np


data = input('Enter a sentence:')

# Word to Vector
X = tfidf.transform([data])

# Classify Prediction
predictions = lr.predict(X)
predictions = predictions[0]
print('Result:', classify(predictions))

with open(r"TFIDFWeights.pkl", "wb") as output_file:
   pkl.dump(tfidf, output_file)